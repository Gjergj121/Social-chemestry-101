{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"metaeval/social-chemestry-101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['area', 'm', 'split', 'rot-agree', 'rot-categorization', 'rot-moral-foundations', 'rot-char-targeting', 'rot-bad', 'rot-judgment', 'action', 'action-agency', 'action-moral-judgment', 'action-agree', 'action-legal', 'action-pressure', 'action-char-involved', 'action-hypothetical', 'situation', 'situation-short-id', 'rot', 'rot-id', 'rot-worker-id', 'breakdown-worker-id', 'n-characters', 'characters'],\n",
       "        num_rows: 355922\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': 'amitheasshole',\n",
       " 'm': 1,\n",
       " 'split': 'train',\n",
       " 'rot-agree': 3.0,\n",
       " 'rot-categorization': 'morality-ethics',\n",
       " 'rot-moral-foundations': 'care-harm|loyalty-betrayal',\n",
       " 'rot-char-targeting': 'char-1',\n",
       " 'rot-bad': 0,\n",
       " 'rot-judgment': \"it's okay\",\n",
       " 'action': 'feeling angry when you find out your roommate lied to you',\n",
       " 'action-agency': 'experience',\n",
       " 'action-moral-judgment': None,\n",
       " 'action-agree': 3.0,\n",
       " 'action-legal': None,\n",
       " 'action-pressure': None,\n",
       " 'action-char-involved': None,\n",
       " 'action-hypothetical': None,\n",
       " 'situation': \"telling my roommate with 2 months notice that I'm going to move out, then after he bought a house to live in, change my mind and getting a new roommate\",\n",
       " 'situation-short-id': 'reddit/amitheasshole/adwxny',\n",
       " 'rot': \"If you find out that your roommate lied to you, it's okay to feel angry.\",\n",
       " 'rot-id': 'rot/reddit/amitheasshole/adwxny/3DH6GAKTY14IXU2D5WHPXGPIFGTZYG/129/4',\n",
       " 'rot-worker-id': 129,\n",
       " 'breakdown-worker-id': 17,\n",
       " 'n-characters': 3,\n",
       " 'characters': 'narrator|my roommate|a new roommate'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'rot' = 'rot-judgement' + 'action'.  1â€“5 RoTs for each 'situation'    (see https://github.com/mbforbes/social-chemistry-101)\n",
    "\n",
    "##### RoT attributes:\n",
    "- rot\n",
    "- rot-agree: asks how many people probably agree with the RoT as stated. How universally held is the rule-of-thumb as a belief?\n",
    "    - 0: almost no one\n",
    "    - 1: uncommon\n",
    "    - 2: contreversial\n",
    "    - 3: common\n",
    "    - 4: universal\n",
    "- rot-categorization: the category of the rule of thumb. (15 in total)\n",
    "- rot-moral-foundations: define fundamental axes of morality. (5 in total)\n",
    "- rot-char-targeting: The relevant character who in the situation is the person to who you would tell this rule-of-thumb. e.g. char-1 = my roommate (characters[1])\n",
    "- rot-bad: 0 or 1, 98% of elements have the value 0\n",
    "- rot-judgement: the judgement of the action\n",
    "\n",
    "##### Action attributes:\n",
    "- action\n",
    "- action-agency: agency or experience (is designed to let workers distinguish RoTs that involve agentive action from those that indicate an an experience)\n",
    "- action-moral-judgment: This is an intuitive reaction of whether something is good or bad.\n",
    "    - -2: very bad\n",
    "    - -1: bad\n",
    "    - 0: expected/OK\n",
    "    - 1: good\n",
    "    - 2: very good\n",
    "- action-agree: what portion of people probably agree with the judgment given the action\n",
    "    - 0: rare (< 1%)\n",
    "    - 1: few (5-25%)\n",
    "    - 2: controversial (~50%)\n",
    "    - 3: most (75-90%)\n",
    "    - 4: all (> 99%)\n",
    "- action-legal: corresponds to prescriptive norms: what one ought to do (legal, tolerated, illegal, null)\n",
    "- action-pressure: cultural pressure, measures to what degree someone feels socially influenced to do (or avoid) an action.\n",
    "    - -2: strongly-against\n",
    "    - -1: against\n",
    "    - 0: discretionary\n",
    "    - 1: for\n",
    "    - 2: strongly for\n",
    "- action-char-involved: \n",
    "- action-hypothetical: indicates whether the candidate character is explicitly taking the action in the situation: explicit-no, probable-no, hypothetical, probable, explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['area', 'm', 'split', 'rot-agree', 'rot-categorization', 'rot-moral-foundations', 'rot-char-targeting', 'rot-bad', 'rot-judgment', 'action', 'action-agency', 'action-moral-judgment', 'action-agree', 'action-legal', 'action-pressure', 'action-char-involved', 'action-hypothetical', 'situation', 'situation-short-id', 'rot', 'rot-id', 'rot-worker-id', 'breakdown-worker-id', 'n-characters', 'characters'],\n",
       "        num_rows: 233501\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset.filter(lambda example: example['split'] == 'train')\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['area', 'm', 'split', 'rot-agree', 'rot-categorization', 'rot-moral-foundations', 'rot-char-targeting', 'rot-bad', 'rot-judgment', 'action', 'action-agency', 'action-moral-judgment', 'action-agree', 'action-legal', 'action-pressure', 'action-char-involved', 'action-hypothetical', 'situation', 'situation-short-id', 'rot', 'rot-id', 'rot-worker-id', 'breakdown-worker-id', 'n-characters', 'characters'],\n",
       "        num_rows: 29239\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset.filter(lambda example: example['split'] == 'test')\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 467002\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 58468\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 58478\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "path = \"/home/IAIS/gplepi/entero/data_social_norms/binary_classification_bert\"\n",
    "bert_dataset = datasets.load_from_disk(path)\n",
    "bert_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NOT-APPLIED\", 1: \"APPLIED\"}\n",
    "\n",
    "label2id = {\"NOT-APPLIED\": 0, \"APPLIED\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda:0')\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_social_norms/bert_binary_classification\",\n",
    "    learning_rate=2e-5,\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=3000,\n",
    "    save_steps=3000,\n",
    "    save_strategy=\"steps\",\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    precision_score = precision.compute(predictions=predictions, references=labels)\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score['precision'],\n",
    "        \"recall\": recall_score['recall'],\n",
    "        \"f1\": f1_score['f1'],\n",
    "        \"accuracy\": accuracy_score['accuracy'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=bert_dataset['train'],\n",
    "    eval_dataset=bert_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgjergjplepi12\u001b[0m (\u001b[33msocial-chem-101-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/IAIS/gplepi/entero/wandb/run-20240102_133918-va9kyr01</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/social-chem-101-team/huggingface/runs/va9kyr01' target=\"_blank\">hardy-frog-6</a></strong> to <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/social-chem-101-team/huggingface/runs/va9kyr01' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface/runs/va9kyr01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13614' max='21891' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13614/21891 4:00:43 < 2:26:22, 0.94 it/s, Epoch 1.87/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.221082</td>\n",
       "      <td>0.933387</td>\n",
       "      <td>0.877608</td>\n",
       "      <td>0.904638</td>\n",
       "      <td>0.907488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.212600</td>\n",
       "      <td>0.193477</td>\n",
       "      <td>0.911729</td>\n",
       "      <td>0.929568</td>\n",
       "      <td>0.920562</td>\n",
       "      <td>0.919785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.186887</td>\n",
       "      <td>0.919100</td>\n",
       "      <td>0.929192</td>\n",
       "      <td>0.924118</td>\n",
       "      <td>0.923702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.188436</td>\n",
       "      <td>0.937261</td>\n",
       "      <td>0.912157</td>\n",
       "      <td>0.924538</td>\n",
       "      <td>0.925549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"/home/IAIS/gplepi/entero/output_social_norms/bert_binary_classification/checkpoint-21000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=finetuned_model, #give here as a parameter the finetuned model\n",
    "    args=training_args,\n",
    "    train_dataset=bert_dataset['train'],\n",
    "    eval_dataset=bert_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.20166319608688354, 'test_precision': 0.9313823377517645, 'test_recall': 0.9252026403091761, 'test_f1': 0.9282822043785602, 'test_accuracy': 0.9285201272273333, 'test_runtime': 476.6797, 'test_samples_per_second': 122.678, 'test_steps_per_second': 1.917}\n"
     ]
    }
   ],
   "source": [
    "predictions_output = trainer.predict(bert_dataset['test'])\n",
    "print(predictions_output.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'APPLIED', 'score': 0.8717881441116333}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "situation = \"Wanting to leave my country\"\n",
    "rot_pos = \"It is fine to do what is best for you\"\n",
    "rot_neg = \"It is bad to not call your mother\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=finetuned_model.to(\"cpu\"), tokenizer=tokenizer)\n",
    "answer = clf(situation + \" \" + tokenizer.sep_token + \" \" + rot_pos)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NOT-APPLIED', 'score': 0.9997960925102234}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = clf(situation + \" \" + tokenizer.sep_token + \" \" + rot_neg)\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entero_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

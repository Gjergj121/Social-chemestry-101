{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # use regex to replace all 'ytr' and 'ntr' with empty spaces \n",
    "    text = re.sub(r'yta|nta', '', text)\n",
    "    # remove all empty spaces \n",
    "    # remove all non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    #remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    #truncate text to at most 600 characters\n",
    "    if len(text) > 600:\n",
    "        text = text[:600]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and preprocess\n",
    "data = pd.read_csv(\"./users_perception/social_comments.csv\")\n",
    "data['body'] = data['body'].apply(preprocess_text)\n",
    "use_data = data[[\"label\", \"body\"]].copy()#use only label and body columns\n",
    "\n",
    "label_mapping = {\"NTA\" : 0, \"YTA\":  1}#add a mapping for the labels\n",
    "use_data[\"label\"] = use_data[\"label\"].map(label_mapping) \n",
    "\n",
    "#split the data to 0.6, 0.2, 0.2\n",
    "train_data, tmp_data = train_test_split(use_data, test_size=0.4, random_state=42)\n",
    "test_data, val_data = train_test_split(tmp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "#fuse all subsets into one dataset:\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset, 'val': val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275f33a94ce641bfb2df5bd0a3cffc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c20055dd8ec4f09ae4198afee62524e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42537 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd74c18aa204062ae4fcc8453d3ad75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42538 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")#load pretrained gpt 2\n",
    "def preprocess(data):\n",
    "    return tokenizer(data[\"body\"], truncation=True)\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "#Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"NTA\", 1: \"YTA\"}\n",
    "label2id = {\"NTA\": 0, \"YTA\": 1}\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'body', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 42538\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/roshdim1/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ca413c224844938e109950caad6a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7656, 'learning_rate': 1.9764916075038793e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6313, 'learning_rate': 1.952983215007758e-05, 'epoch': 0.05}\n",
      "{'loss': 0.6322, 'learning_rate': 1.9294748225116368e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6215, 'learning_rate': 1.9059664300155156e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6069, 'learning_rate': 1.8824580375193947e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5987, 'learning_rate': 1.8589496450232734e-05, 'epoch': 0.14}\n",
      "{'loss': 0.596, 'learning_rate': 1.8354412525271522e-05, 'epoch': 0.16}\n",
      "{'loss': 0.5886, 'learning_rate': 1.8119328600310313e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5804, 'learning_rate': 1.78842446753491e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5846, 'learning_rate': 1.764916075038789e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5916, 'learning_rate': 1.741407682542668e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5792, 'learning_rate': 1.7178992900465467e-05, 'epoch': 0.28}\n",
      "{'loss': 0.574, 'learning_rate': 1.6943908975504258e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5729, 'learning_rate': 1.6708825050543046e-05, 'epoch': 0.33}\n",
      "{'loss': 0.5722, 'learning_rate': 1.6473741125581833e-05, 'epoch': 0.35}\n",
      "{'loss': 0.5596, 'learning_rate': 1.623865720062062e-05, 'epoch': 0.38}\n",
      "{'loss': 0.5756, 'learning_rate': 1.6003573275659412e-05, 'epoch': 0.4}\n",
      "{'loss': 0.554, 'learning_rate': 1.57684893506982e-05, 'epoch': 0.42}\n",
      "{'loss': 0.5624, 'learning_rate': 1.553340542573699e-05, 'epoch': 0.45}\n",
      "{'loss': 0.5483, 'learning_rate': 1.5298321500775778e-05, 'epoch': 0.47}\n",
      "{'loss': 0.5475, 'learning_rate': 1.5063237575814566e-05, 'epoch': 0.49}\n",
      "{'loss': 0.5494, 'learning_rate': 1.4828153650853357e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5365, 'learning_rate': 1.4593069725892145e-05, 'epoch': 0.54}\n",
      "{'loss': 0.546, 'learning_rate': 1.4357985800930934e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5458, 'learning_rate': 1.4122901875969723e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5299, 'learning_rate': 1.388781795100851e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5439, 'learning_rate': 1.36527340260473e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5444, 'learning_rate': 1.3417650101086088e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5244, 'learning_rate': 1.3182566176124879e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5317, 'learning_rate': 1.2947482251163665e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5274, 'learning_rate': 1.2712398326202456e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5441, 'learning_rate': 1.2477314401241245e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5283, 'learning_rate': 1.2242230476280033e-05, 'epoch': 0.78}\n",
      "{'loss': 0.512, 'learning_rate': 1.2007146551318822e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5109, 'learning_rate': 1.177206262635761e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5371, 'learning_rate': 1.1536978701396399e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5141, 'learning_rate': 1.1301894776435187e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5304, 'learning_rate': 1.1066810851473978e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5296, 'learning_rate': 1.0831726926512767e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5344, 'learning_rate': 1.0596643001551555e-05, 'epoch': 0.94}\n",
      "{'loss': 0.5166, 'learning_rate': 1.0361559076590344e-05, 'epoch': 0.96}\n",
      "{'loss': 0.5094, 'learning_rate': 1.0126475151629132e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d46c7b0ad5420b90a001bbe32f1d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4974004626274109, 'eval_accuracy': 0.7702470790135647, 'eval_runtime': 91.4257, 'eval_samples_per_second': 465.263, 'eval_steps_per_second': 77.549, 'epoch': 1.0}\n",
      "{'loss': 0.5165, 'learning_rate': 9.891391226667921e-06, 'epoch': 1.01}\n",
      "{'loss': 0.5017, 'learning_rate': 9.65630730170671e-06, 'epoch': 1.03}\n",
      "{'loss': 0.5128, 'learning_rate': 9.421223376745498e-06, 'epoch': 1.06}\n",
      "{'loss': 0.4945, 'learning_rate': 9.186139451784287e-06, 'epoch': 1.08}\n",
      "{'loss': 0.4982, 'learning_rate': 8.951055526823077e-06, 'epoch': 1.1}\n",
      "{'loss': 0.4971, 'learning_rate': 8.715971601861866e-06, 'epoch': 1.13}\n",
      "{'loss': 0.484, 'learning_rate': 8.480887676900654e-06, 'epoch': 1.15}\n",
      "{'loss': 0.5148, 'learning_rate': 8.245803751939443e-06, 'epoch': 1.18}\n",
      "{'loss': 0.5104, 'learning_rate': 8.010719826978232e-06, 'epoch': 1.2}\n",
      "{'loss': 0.4905, 'learning_rate': 7.77563590201702e-06, 'epoch': 1.22}\n",
      "{'loss': 0.5013, 'learning_rate': 7.54055197705581e-06, 'epoch': 1.25}\n",
      "{'loss': 0.4979, 'learning_rate': 7.305468052094599e-06, 'epoch': 1.27}\n",
      "{'loss': 0.4809, 'learning_rate': 7.070384127133387e-06, 'epoch': 1.29}\n",
      "{'loss': 0.4743, 'learning_rate': 6.835300202172176e-06, 'epoch': 1.32}\n",
      "{'loss': 0.4878, 'learning_rate': 6.600216277210965e-06, 'epoch': 1.34}\n",
      "{'loss': 0.4821, 'learning_rate': 6.3651323522497535e-06, 'epoch': 1.36}\n",
      "{'loss': 0.489, 'learning_rate': 6.130048427288543e-06, 'epoch': 1.39}\n",
      "{'loss': 0.4796, 'learning_rate': 5.894964502327331e-06, 'epoch': 1.41}\n",
      "{'loss': 0.4838, 'learning_rate': 5.659880577366121e-06, 'epoch': 1.43}\n",
      "{'loss': 0.4839, 'learning_rate': 5.424796652404909e-06, 'epoch': 1.46}\n",
      "{'loss': 0.496, 'learning_rate': 5.189712727443698e-06, 'epoch': 1.48}\n",
      "{'loss': 0.4953, 'learning_rate': 4.954628802482487e-06, 'epoch': 1.5}\n",
      "{'loss': 0.4646, 'learning_rate': 4.7195448775212754e-06, 'epoch': 1.53}\n",
      "{'loss': 0.4743, 'learning_rate': 4.484460952560064e-06, 'epoch': 1.55}\n",
      "{'loss': 0.4903, 'learning_rate': 4.249377027598853e-06, 'epoch': 1.58}\n",
      "{'loss': 0.4851, 'learning_rate': 4.014293102637642e-06, 'epoch': 1.6}\n",
      "{'loss': 0.4934, 'learning_rate': 3.7792091776764307e-06, 'epoch': 1.62}\n",
      "{'loss': 0.4812, 'learning_rate': 3.5441252527152196e-06, 'epoch': 1.65}\n",
      "{'loss': 0.4846, 'learning_rate': 3.3090413277540085e-06, 'epoch': 1.67}\n",
      "{'loss': 0.4747, 'learning_rate': 3.073957402792797e-06, 'epoch': 1.69}\n",
      "{'loss': 0.4835, 'learning_rate': 2.8388734778315863e-06, 'epoch': 1.72}\n",
      "{'loss': 0.473, 'learning_rate': 2.603789552870375e-06, 'epoch': 1.74}\n",
      "{'loss': 0.4955, 'learning_rate': 2.3687056279091637e-06, 'epoch': 1.76}\n",
      "{'loss': 0.4866, 'learning_rate': 2.1336217029479526e-06, 'epoch': 1.79}\n",
      "{'loss': 0.4764, 'learning_rate': 1.8985377779867413e-06, 'epoch': 1.81}\n",
      "{'loss': 0.487, 'learning_rate': 1.6634538530255305e-06, 'epoch': 1.83}\n",
      "{'loss': 0.4992, 'learning_rate': 1.428369928064319e-06, 'epoch': 1.86}\n",
      "{'loss': 0.465, 'learning_rate': 1.1932860031031079e-06, 'epoch': 1.88}\n",
      "{'loss': 0.5036, 'learning_rate': 9.582020781418968e-07, 'epoch': 1.9}\n",
      "{'loss': 0.4945, 'learning_rate': 7.231181531806855e-07, 'epoch': 1.93}\n",
      "{'loss': 0.4853, 'learning_rate': 4.880342282194744e-07, 'epoch': 1.95}\n",
      "{'loss': 0.4788, 'learning_rate': 2.5295030325826324e-07, 'epoch': 1.97}\n",
      "{'loss': 0.4543, 'learning_rate': 1.786637829705205e-08, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851d57fff6884a18af7281f619199732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5007114410400391, 'eval_accuracy': 0.7844464818863578, 'eval_runtime': 90.9365, 'eval_samples_per_second': 467.766, 'eval_steps_per_second': 77.967, 'epoch': 2.0}\n",
      "{'train_runtime': 2464.7725, 'train_samples_per_second': 103.549, 'train_steps_per_second': 17.258, 'train_loss': 0.5245063800256664, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=42538, training_loss=0.5245063800256664, metrics={'train_runtime': 2464.7725, 'train_samples_per_second': 103.549, 'train_steps_per_second': 17.258, 'train_loss': 0.5245063800256664, 'epoch': 2.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "import numpy as np\n",
    "f1_scores = []\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    #add f1 metric\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    return accuracy.compute(predictions=predictions, references=labels) \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2_aita\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    #data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bd90e786d14e739f8a1897bdac57f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4974004626274109,\n",
       " 'eval_accuracy': 0.7702470790135647,\n",
       " 'eval_runtime': 91.1727,\n",
       " 'eval_samples_per_second': 466.554,\n",
       " 'eval_steps_per_second': 77.764,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get f1 score of eval dataset\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'f1': 0.5240808375943511},\n",
       " {'f1': 0.5465156535931549},\n",
       " {'f1': 0.5240808375943511}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_unpacked",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Comments Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>label</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>eamexog</td>\n",
       "      <td>/r/AmItheAsshole/comments/a1311q/aita_for_tell...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>Uh absolutely NTA. These are really really hor...</td>\n",
       "      <td>a1311q</td>\n",
       "      <td>t2_2kabg9z7</td>\n",
       "      <td>xormun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>eameha5</td>\n",
       "      <td>/r/AmItheAsshole/comments/a1311q/aita_for_tell...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>NTA. Ok sweetie no, hell no this is not your f...</td>\n",
       "      <td>a1311q</td>\n",
       "      <td>t2_1jrodkow</td>\n",
       "      <td>tkPuncake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eamjnog</td>\n",
       "      <td>/r/AmItheAsshole/comments/a1311q/aita_for_tell...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>NTA. My girlfriend has hypothyroidism and i kn...</td>\n",
       "      <td>a1311q</td>\n",
       "      <td>t2_14ub01</td>\n",
       "      <td>hawkbearpig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ef5kbsb</td>\n",
       "      <td>/r/AmItheAsshole/comments/akkcpn/aita_for_not_...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>You're clearly NTA. Sorry about your homophobi...</td>\n",
       "      <td>akkcpn</td>\n",
       "      <td>t2_61b3s</td>\n",
       "      <td>sadsquash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ef5l208</td>\n",
       "      <td>/r/AmItheAsshole/comments/akkcpn/aita_for_not_...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>NTA. And it will get better, I promise. You'll...</td>\n",
       "      <td>akkcpn</td>\n",
       "      <td>t2_xvrsh</td>\n",
       "      <td>SheketBevakaSTFU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530631</th>\n",
       "      <td>ei9ofvo</td>\n",
       "      <td>/r/AmItheAsshole/comments/azofrl/aita_for_not_...</td>\n",
       "      <td>YTA</td>\n",
       "      <td>NAH (a bit towards yta) OP. You found the wors...</td>\n",
       "      <td>azofrl</td>\n",
       "      <td>t2_16fctm</td>\n",
       "      <td>xAlois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530636</th>\n",
       "      <td>ei9gkon</td>\n",
       "      <td>/r/AmItheAsshole/comments/azofrl/aita_for_not_...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>NTA &amp;#x200B; but you handled it really poorly,...</td>\n",
       "      <td>azofrl</td>\n",
       "      <td>t2_dk4gojr</td>\n",
       "      <td>YoungDiscord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530637</th>\n",
       "      <td>ei9gl79</td>\n",
       "      <td>/r/AmItheAsshole/comments/azofrl/aita_for_not_...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>NTA how are you the asshole? For like bigger b...</td>\n",
       "      <td>azofrl</td>\n",
       "      <td>t2_2xfoz1fv</td>\n",
       "      <td>Dark-_-Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530639</th>\n",
       "      <td>ei9gmpk</td>\n",
       "      <td>/r/AmItheAsshole/comments/azofrl/aita_for_not_...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>NTA. Your girlfriend is overreacting. You don'...</td>\n",
       "      <td>azofrl</td>\n",
       "      <td>t2_15bdqt5w</td>\n",
       "      <td>Broken_Angel-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530660</th>\n",
       "      <td>ei9bv68</td>\n",
       "      <td>/r/AmItheAsshole/comments/azofrl/aita_for_not_...</td>\n",
       "      <td>YTA</td>\n",
       "      <td>YTA if it's that big of a problem, but her som...</td>\n",
       "      <td>azofrl</td>\n",
       "      <td>t2_21p0gvq6</td>\n",
       "      <td>op2mus_2357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212687 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          permalink label  \\\n",
       "13      eamexog  /r/AmItheAsshole/comments/a1311q/aita_for_tell...   NTA   \n",
       "14      eameha5  /r/AmItheAsshole/comments/a1311q/aita_for_tell...   NTA   \n",
       "16      eamjnog  /r/AmItheAsshole/comments/a1311q/aita_for_tell...   NTA   \n",
       "17      ef5kbsb  /r/AmItheAsshole/comments/akkcpn/aita_for_not_...   NTA   \n",
       "20      ef5l208  /r/AmItheAsshole/comments/akkcpn/aita_for_not_...   NTA   \n",
       "...         ...                                                ...   ...   \n",
       "530631  ei9ofvo  /r/AmItheAsshole/comments/azofrl/aita_for_not_...   YTA   \n",
       "530636  ei9gkon  /r/AmItheAsshole/comments/azofrl/aita_for_not_...   NTA   \n",
       "530637  ei9gl79  /r/AmItheAsshole/comments/azofrl/aita_for_not_...   NTA   \n",
       "530639  ei9gmpk  /r/AmItheAsshole/comments/azofrl/aita_for_not_...   NTA   \n",
       "530660  ei9bv68  /r/AmItheAsshole/comments/azofrl/aita_for_not_...   YTA   \n",
       "\n",
       "                                                     body parent_id  \\\n",
       "13      Uh absolutely NTA. These are really really hor...    a1311q   \n",
       "14      NTA. Ok sweetie no, hell no this is not your f...    a1311q   \n",
       "16      NTA. My girlfriend has hypothyroidism and i kn...    a1311q   \n",
       "17      You're clearly NTA. Sorry about your homophobi...    akkcpn   \n",
       "20      NTA. And it will get better, I promise. You'll...    akkcpn   \n",
       "...                                                   ...       ...   \n",
       "530631  NAH (a bit towards yta) OP. You found the wors...    azofrl   \n",
       "530636  NTA &#x200B; but you handled it really poorly,...    azofrl   \n",
       "530637  NTA how are you the asshole? For like bigger b...    azofrl   \n",
       "530639  NTA. Your girlfriend is overreacting. You don'...    azofrl   \n",
       "530660  YTA if it's that big of a problem, but her som...    azofrl   \n",
       "\n",
       "       author_fullname       author_name  \n",
       "13         t2_2kabg9z7            xormun  \n",
       "14         t2_1jrodkow         tkPuncake  \n",
       "16           t2_14ub01       hawkbearpig  \n",
       "17            t2_61b3s         sadsquash  \n",
       "20            t2_xvrsh  SheketBevakaSTFU  \n",
       "...                ...               ...  \n",
       "530631       t2_16fctm            xAlois  \n",
       "530636      t2_dk4gojr      YoungDiscord  \n",
       "530637     t2_2xfoz1fv     Dark-_-Legacy  \n",
       "530639     t2_15bdqt5w     Broken_Angel-  \n",
       "530660     t2_21p0gvq6       op2mus_2357  \n",
       "\n",
       "[212687 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_comments = pd.read_pickle('/home/IAIS/gplepi/entero/data_social_norms/social_comments_filtered.gzip', compression='gzip')\n",
    "social_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any\n",
    "import string\n",
    "\n",
    "\"\"\"\n",
    "Text preprocessing: lowercase; remove punctuation; remove NTA_KEYWORDS, YTA_KEYWORDS and 'ampx200b', 'x200b', 'AITA', 'aita'\n",
    "\"\"\"\n",
    "\n",
    "class KeywordsCleaner:\n",
    "    def __init__(self) -> None:\n",
    "        # NTA YTA keywords\n",
    "        NTA_KEYWORDS = ['nta', 'nah', 'you are not the asshole', 'you\\'re not the asshole', 'u are not the asshole', 'u re not the asshole', \n",
    "                        'you re not the asshole', 'u\\'re not the asshole', 'not the asshole', 'not the ah', 'not asshole', 'not ah']\n",
    "        YTA_KEYWORDS = ['yta', 'you are the asshole', 'you\\'re the asshole', 'u are the asshole', 'u re the asshole', \n",
    "                        'you re the asshole', 'u\\'re the asshole', 'you the ah', 'you the asshole', 'u the asshole', 'u the ah']\n",
    "\n",
    "        keywords_rep = {'ampx200b': \"\", 'x200b': \"\", 'AITA': \"\", 'aita': \"\"}\n",
    "        \n",
    "        for key in NTA_KEYWORDS + YTA_KEYWORDS:\n",
    "            keywords_rep[key] = \"\"\n",
    "        keywords_rep = dict(sorted(keywords_rep.items(), key=lambda k: len(k[0]), reverse=True))\n",
    "\n",
    "        self.rep = dict((re.escape(k), v) for k, v in keywords_rep.items())\n",
    "        self.pattern = re.compile(\"|\".join(self.rep.keys()))\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        text = self.pattern.sub(lambda m: self.rep[re.escape(m.group(0))], text.lower())\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTA &#x200B; but you handled it really poorly, like that was the absolute worst way in which you could have ever said it &#x200B; why not acknowledge that she's beautiful and sexy in her own way? make a lateral move that you know, wouldn't require you to directly say: I'm not attracted to you physically? because that would just open up a can of worms. &#x200B; well, you messed it up so now you have to fix it. &#x200B; You're not an asshole for having a personal body type preference, everyone has but you are an idiot for handling it the way you did, good luck with that.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  but you handled it really poorly like that was the absolute worst way in which you could have ever said it  why not acknowledge that shes beautiful and sexy in her own way make a lateral move that you know wouldnt require you to directly say im not attracted to you physically because that would just open up a can of worms  well you messed it up so now you have to fix it  youre not an asshole for having a personal body type preference everyone has but you are an idiot for handling it the way you did good luck with that'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Example\"\"\"\n",
    "keywordsCleaner = KeywordsCleaner()\n",
    "\n",
    "print(social_comments[\"body\"].at[530636])\n",
    "keywordsCleaner(social_comments[\"body\"].at[530636])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>label</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>eamexog</td>\n",
       "      <td>/r/AmItheAsshole/comments/a1311q/aita_for_tell...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>uh absolutely  these are really really horrid ...</td>\n",
       "      <td>a1311q</td>\n",
       "      <td>t2_2kabg9z7</td>\n",
       "      <td>xormun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>eameha5</td>\n",
       "      <td>/r/AmItheAsshole/comments/a1311q/aita_for_tell...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>ok sweetie no hell no this is not your fault ...</td>\n",
       "      <td>a1311q</td>\n",
       "      <td>t2_1jrodkow</td>\n",
       "      <td>tkPuncake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eamjnog</td>\n",
       "      <td>/r/AmItheAsshole/comments/a1311q/aita_for_tell...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>my girlfriend has hypothyroidism and i know t...</td>\n",
       "      <td>a1311q</td>\n",
       "      <td>t2_14ub01</td>\n",
       "      <td>hawkbearpig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ef5kbsb</td>\n",
       "      <td>/r/AmItheAsshole/comments/akkcpn/aita_for_not_...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>youre clearly  sorry about your homophobic fam...</td>\n",
       "      <td>akkcpn</td>\n",
       "      <td>t2_61b3s</td>\n",
       "      <td>sadsquash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ef5l208</td>\n",
       "      <td>/r/AmItheAsshole/comments/akkcpn/aita_for_not_...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>and it will get better i promise youll make i...</td>\n",
       "      <td>akkcpn</td>\n",
       "      <td>t2_xvrsh</td>\n",
       "      <td>SheketBevakaSTFU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530631</th>\n",
       "      <td>ei9ofvo</td>\n",
       "      <td>/r/AmItheAsshole/comments/azofrl/aita_for_not_...</td>\n",
       "      <td>YTA</td>\n",
       "      <td>a bit towards  op you found the worst way to ...</td>\n",
       "      <td>azofrl</td>\n",
       "      <td>t2_16fctm</td>\n",
       "      <td>xAlois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530636</th>\n",
       "      <td>ei9gkon</td>\n",
       "      <td>/r/AmItheAsshole/comments/azofrl/aita_for_not_...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>but you handled it really poorly like that w...</td>\n",
       "      <td>azofrl</td>\n",
       "      <td>t2_dk4gojr</td>\n",
       "      <td>YoungDiscord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530637</th>\n",
       "      <td>ei9gl79</td>\n",
       "      <td>/r/AmItheAsshole/comments/azofrl/aita_for_not_...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>how are  for like bigger boobs</td>\n",
       "      <td>azofrl</td>\n",
       "      <td>t2_2xfoz1fv</td>\n",
       "      <td>Dark-_-Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530639</th>\n",
       "      <td>ei9gmpk</td>\n",
       "      <td>/r/AmItheAsshole/comments/azofrl/aita_for_not_...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>your girlfriend is overreacting you dont have...</td>\n",
       "      <td>azofrl</td>\n",
       "      <td>t2_15bdqt5w</td>\n",
       "      <td>Broken_Angel-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530660</th>\n",
       "      <td>ei9bv68</td>\n",
       "      <td>/r/AmItheAsshole/comments/azofrl/aita_for_not_...</td>\n",
       "      <td>YTA</td>\n",
       "      <td>if its that big of a problem but her some new...</td>\n",
       "      <td>azofrl</td>\n",
       "      <td>t2_21p0gvq6</td>\n",
       "      <td>op2mus_2357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212687 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          permalink label  \\\n",
       "13      eamexog  /r/AmItheAsshole/comments/a1311q/aita_for_tell...   NTA   \n",
       "14      eameha5  /r/AmItheAsshole/comments/a1311q/aita_for_tell...   NTA   \n",
       "16      eamjnog  /r/AmItheAsshole/comments/a1311q/aita_for_tell...   NTA   \n",
       "17      ef5kbsb  /r/AmItheAsshole/comments/akkcpn/aita_for_not_...   NTA   \n",
       "20      ef5l208  /r/AmItheAsshole/comments/akkcpn/aita_for_not_...   NTA   \n",
       "...         ...                                                ...   ...   \n",
       "530631  ei9ofvo  /r/AmItheAsshole/comments/azofrl/aita_for_not_...   YTA   \n",
       "530636  ei9gkon  /r/AmItheAsshole/comments/azofrl/aita_for_not_...   NTA   \n",
       "530637  ei9gl79  /r/AmItheAsshole/comments/azofrl/aita_for_not_...   NTA   \n",
       "530639  ei9gmpk  /r/AmItheAsshole/comments/azofrl/aita_for_not_...   NTA   \n",
       "530660  ei9bv68  /r/AmItheAsshole/comments/azofrl/aita_for_not_...   YTA   \n",
       "\n",
       "                                                     body parent_id  \\\n",
       "13      uh absolutely  these are really really horrid ...    a1311q   \n",
       "14       ok sweetie no hell no this is not your fault ...    a1311q   \n",
       "16       my girlfriend has hypothyroidism and i know t...    a1311q   \n",
       "17      youre clearly  sorry about your homophobic fam...    akkcpn   \n",
       "20       and it will get better i promise youll make i...    akkcpn   \n",
       "...                                                   ...       ...   \n",
       "530631   a bit towards  op you found the worst way to ...    azofrl   \n",
       "530636    but you handled it really poorly like that w...    azofrl   \n",
       "530637                     how are  for like bigger boobs    azofrl   \n",
       "530639   your girlfriend is overreacting you dont have...    azofrl   \n",
       "530660   if its that big of a problem but her some new...    azofrl   \n",
       "\n",
       "       author_fullname       author_name  \n",
       "13         t2_2kabg9z7            xormun  \n",
       "14         t2_1jrodkow         tkPuncake  \n",
       "16           t2_14ub01       hawkbearpig  \n",
       "17            t2_61b3s         sadsquash  \n",
       "20            t2_xvrsh  SheketBevakaSTFU  \n",
       "...                ...               ...  \n",
       "530631       t2_16fctm            xAlois  \n",
       "530636      t2_dk4gojr      YoungDiscord  \n",
       "530637     t2_2xfoz1fv     Dark-_-Legacy  \n",
       "530639     t2_15bdqt5w     Broken_Angel-  \n",
       "530660     t2_21p0gvq6       op2mus_2357  \n",
       "\n",
       "[212687 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Filter social comments\"\"\"\n",
    "keywordsCleaner = KeywordsCleaner()\n",
    "\n",
    "for i, row in social_comments.iterrows():\n",
    "    row['body'] = keywordsCleaner(row['body'])\n",
    "\n",
    "social_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'permalink', 'label', 'body', 'parent_id', 'author_fullname', 'author_name', '__index_level_0__'],\n",
       "    num_rows: 212687\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"From pandas to Huggingface Dataset\"\"\"\n",
    "from datasets import Dataset\n",
    "\n",
    "social_comments_dataset = Dataset.from_pandas(social_comments)\n",
    "social_comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212687/212687 [00:09<00:00, 21686.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NTA': 150040, 'YTA': 62647}\n",
      "NTA: 0.7054497924179663 \n",
      "YTA: 0.2945502075820337 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Analyze the data\"\"\"\n",
    "data_distribution = {'NTA': 0, 'YTA': 0}\n",
    "\n",
    "def compute_data_distribution(example):\n",
    "    data_distribution[example['label']] += 1\n",
    "\n",
    "social_comments_dataset.map(compute_data_distribution)\n",
    "\n",
    "print(data_distribution)\n",
    "print(f\"NTA: {data_distribution['NTA'] / sum(data_distribution.values())} \")\n",
    "print(f\"YTA: {data_distribution['YTA'] / sum(data_distribution.values())} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'permalink', 'label', 'body', 'parent_id', 'author_fullname', 'author_name', '__index_level_0__'],\n",
       "        num_rows: 170149\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'permalink', 'label', 'body', 'parent_id', 'author_fullname', 'author_name', '__index_level_0__'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'permalink', 'label', 'body', 'parent_id', 'author_fullname', 'author_name', '__index_level_0__'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "\"\"\"80-10-10 split\"\"\"\n",
    "# 80% train, 20% test + validation\n",
    "train_testvalid = social_comments_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "# Split the 20% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "social_comments_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'val': test_valid['train']})\n",
    "\n",
    "social_comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NTA\", 1: \"YTA\"}\n",
    "\n",
    "label2id = {\"NTA\": 0, \"YTA\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# social-chemestry-101 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['area', 'm', 'split', 'rot-agree', 'rot-categorization', 'rot-moral-foundations', 'rot-char-targeting', 'rot-bad', 'rot-judgment', 'action', 'action-agency', 'action-moral-judgment', 'action-agree', 'action-legal', 'action-pressure', 'action-char-involved', 'action-hypothetical', 'situation', 'situation-short-id', 'rot', 'rot-id', 'rot-worker-id', 'breakdown-worker-id', 'n-characters', 'characters'],\n",
       "        num_rows: 355922\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "social_chemestry_dataset = load_dataset(\"metaeval/social-chemestry-101\")\n",
    "social_chemestry_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "situationId_to_situation = {}\n",
    "situationId_to_ROT_moral_foundations = defaultdict(set)\n",
    "situationId_to_ROT_categories = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 355922/355922 [00:45<00:00, 7769.94 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['area', 'm', 'split', 'rot-agree', 'rot-categorization', 'rot-moral-foundations', 'rot-char-targeting', 'rot-bad', 'rot-judgment', 'action', 'action-agency', 'action-moral-judgment', 'action-agree', 'action-legal', 'action-pressure', 'action-char-involved', 'action-hypothetical', 'situation', 'situation-short-id', 'rot', 'rot-id', 'rot-worker-id', 'breakdown-worker-id', 'n-characters', 'characters'],\n",
       "        num_rows: 355922\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapping_from_situationId(example):\n",
    "    situation_id = example['situation-short-id'].split(\"/\")[-1]\n",
    "\n",
    "    situationId_to_situation[situation_id] = example['situation']\n",
    "    if example['rot-moral-foundations'] is not None:\n",
    "        situationId_to_ROT_moral_foundations[situation_id].add(example['rot-moral-foundations'])\n",
    "    if example['rot-categorization'] is not None:\n",
    "        situationId_to_ROT_categories[situation_id].add(example['rot-categorization'])\n",
    "\n",
    "social_chemestry_dataset.map(mapping_from_situationId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'care-harm|fairness-cheating',\n",
       " 'care-harm|loyalty-betrayal',\n",
       " 'fairness-cheating',\n",
       " 'loyalty-betrayal'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "situationId_to_ROT_moral_foundations['adwxny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loyalty-betrayal. fairness-cheating. care-harm|fairness-cheating. care-harm|loyalty-betrayal'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'. '.join(situationId_to_ROT_moral_foundations['adwxny'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization for p(y|c) -> Only comments texts (filtered from NTA/YTA tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_data_only_comments(example):\n",
    "    encoding = tokenizer(example['body'], padding=\"max_length\", truncation=True)\n",
    "    encoding['labels'] = label2id[ example['label'] ]\n",
    "\n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization for p(y|c,s) -> Situation Text + tokenizer.sep_token + comments text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_data_situations_comments(example):\n",
    "    situation = situationId_to_situation[example['parent_id']]\n",
    "    \n",
    "    encoding = tokenizer(situation, example['body'], padding=\"max_length\", truncation=True)\n",
    "    encoding['labels'] = label2id[ example['label'] ]\n",
    "\n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization for p(y|c,rot) -> Rot-moral-foundations +  tokenizer.sep_token + comments text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_data_moralFoundations_comments(example):\n",
    "    rots = '. '.join(situationId_to_ROT_moral_foundations[example['parent_id']])\n",
    "    \n",
    "    encoding = tokenizer(rots, example['body'], padding=\"max_length\", truncation=True)\n",
    "    encoding['labels'] = label2id[ example['label'] ]\n",
    "\n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization for p(y|c,rot) -> Rot-categories +  tokenizer.sep_token + comments text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_data_categories_comments(example):\n",
    "    rots = '. '.join(situationId_to_ROT_categories[example['parent_id']])\n",
    "    \n",
    "    encoding = tokenizer(rots, example['body'], padding=\"max_length\", truncation=True)\n",
    "    encoding['labels'] = label2id[ example['label'] ]\n",
    "\n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization for p(y|c,a) -> Author ID + Comment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_data_authorId_comments(example):\n",
    "\n",
    "    text = example['id'] + \". \" + example['body'] \n",
    "\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True)\n",
    "    encoding['labels'] = label2id[ example['label'] ]\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization for p(y|c,s,a) -> situation + tokenizer.sep_token + (Author ID + Comment text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_data_situation_authorId_comments(example):\n",
    "    situation = situationId_to_situation[example['parent_id']]\n",
    "    text = example['id'] + \". \" + example['body'] \n",
    "\n",
    "    encoding = tokenizer(text, situation, padding=\"max_length\", truncation=True)\n",
    "    encoding['labels'] = label2id[ example['label'] ]\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluation\"\"\"\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\", average='macro')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    precision_score = precision.compute(predictions=predictions, references=labels)\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score['precision'],\n",
    "        \"recall\": recall_score['recall'],\n",
    "        \"f1\": f1_score['f1'],\n",
    "        \"accuracy\": accuracy_score['accuracy'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model for p(y|c) -> only comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 454/170149 [00:00<01:51, 1521.44 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170149/170149 [01:56<00:00, 1459.09 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:15<00:00, 1410.60 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:15<00:00, 1406.97 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 170149\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Tokenize the dataset\"\"\"\n",
    "tokenized_dataset = social_comments_dataset.map(tokenize_data_only_comments)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_social_norms/bert_comments_classification_custom_trainer\",\n",
    "    learning_rate=2e-5,\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119945 training samples have label 0 (NTA), 50204 training samples have label 1 (YTA)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2951, 0.7049])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "We have 119945 training samples with label 0 (NTA) and 50204 training samples with label 1 (YTA). \n",
    "A clear disproportion, thats why we need to assign different weights to the labels when computing the loss. \n",
    "The label with the least amount of data (label 1 in our case), should have the higher weight.\n",
    "\"\"\"\n",
    "\n",
    "def get_samples_per_class(labels):\n",
    "    return torch.bincount(labels)\n",
    "\n",
    "samples_per_class_train = get_samples_per_class(torch.Tensor(tokenized_dataset[\"train\"]['labels']).int())\n",
    "print(f\"{samples_per_class_train[0]} training samples have label 0 (NTA), {samples_per_class_train[1]} training samples have label 1 (YTA)\")\n",
    "\n",
    "total_training_samples = samples_per_class_train.sum()\n",
    "weights = torch.Tensor( [samples_per_class_train[1]/total_training_samples, samples_per_class_train[0]/total_training_samples] )\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "\"\"\"Create Custom Trainer to assign different weights to classes when computing the loss due to the large data imbalance\"\"\"\n",
    "class CustomTrainer(Trainer):\n",
    "    #override the compute_loss function of the Trainer class\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights.to(model.module.device)) # weights is a 1D Tensor assigning weight to each of the classes\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgjergjplepi12\u001b[0m (\u001b[33msocial-chem-101-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/IAIS/gplepi/entero/wandb/run-20240120_111424-cqnypy6i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/social-chem-101-team/huggingface/runs/cqnypy6i' target=\"_blank\">still-sky-39</a></strong> to <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/social-chem-101-team/huggingface/runs/cqnypy6i' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface/runs/cqnypy6i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3990' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3990/3990 1:53:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.419685</td>\n",
       "      <td>0.621259</td>\n",
       "      <td>0.785004</td>\n",
       "      <td>0.693598</td>\n",
       "      <td>0.799944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>0.396550</td>\n",
       "      <td>0.671423</td>\n",
       "      <td>0.782396</td>\n",
       "      <td>0.722674</td>\n",
       "      <td>0.826790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.389822</td>\n",
       "      <td>0.636352</td>\n",
       "      <td>0.840587</td>\n",
       "      <td>0.724349</td>\n",
       "      <td>0.815459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3990, training_loss=0.3838732994289924, metrics={'train_runtime': 6818.1287, 'train_samples_per_second': 74.866, 'train_steps_per_second': 0.585, 'total_flos': 6.761758624175923e+16, 'train_loss': 0.3838732994289924, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86     14961\n",
      "           1       0.64      0.85      0.73      6308\n",
      "\n",
      "    accuracy                           0.82     21269\n",
      "   macro avg       0.78      0.83      0.80     21269\n",
      "weighted avg       0.84      0.82      0.82     21269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Testing\"\"\"\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_path = \"/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_custom_trainer\"\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\"\"\"Predict\"\"\"\n",
    "predictions_output = trainer.predict(tokenized_dataset['test'])\n",
    "\n",
    "\n",
    "preds = np.argmax(predictions_output.predictions, axis=-1)\n",
    "labels = predictions_output.label_ids\n",
    "print(classification_report(y_true=labels, y_pred=preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model for p(y|c,s) -> situations and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170149/170149 [02:11<00:00, 1289.75 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:16<00:00, 1262.53 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:16<00:00, 1254.75 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 170149\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenized_dataset = social_comments_dataset.map(tokenize_data_situations_comments)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_social_norms/bert_comments_classification_situations_and_comments_custom_trainer\",\n",
    "    learning_rate=2e-5,\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119945 training samples have label 0 (NTA), 50204 training samples have label 1 (YTA)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2951, 0.7049])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "We have 119945 training samples with label 0 (NTA) and 50204 training samples with label 1 (YTA). \n",
    "A clear disproportion, thats why we need to assign different weights to the labels when computing the loss. \n",
    "The label with the least amount of data (label 1 in our case), should have the higher weight.\n",
    "\"\"\"\n",
    "\n",
    "def get_samples_per_class(labels):\n",
    "    return torch.bincount(labels)\n",
    "\n",
    "samples_per_class_train = get_samples_per_class(torch.Tensor(tokenized_dataset[\"train\"]['labels']).int())\n",
    "print(f\"{samples_per_class_train[0]} training samples have label 0 (NTA), {samples_per_class_train[1]} training samples have label 1 (YTA)\")\n",
    "\n",
    "total_training_samples = samples_per_class_train.sum()\n",
    "weights = torch.Tensor( [samples_per_class_train[1]/total_training_samples, samples_per_class_train[0]/total_training_samples] )\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "\"\"\"Create Custom Trainer to assign different weights to classes when computing the loss due to the large data imbalance\"\"\"\n",
    "class CustomTrainer(Trainer):\n",
    "    #override the compute_loss function of the Trainer class\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights.to(model.module.device)) # weights is a 1D Tensor assigning weight to each of the classes\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgjergjplepi12\u001b[0m (\u001b[33msocial-chem-101-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/IAIS/gplepi/entero/wandb/run-20240120_150302-5uyr1de4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/social-chem-101-team/huggingface/runs/5uyr1de4' target=\"_blank\">kind-meadow-40</a></strong> to <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/social-chem-101-team/huggingface/runs/5uyr1de4' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface/runs/5uyr1de4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3990' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3990/3990 1:52:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.397700</td>\n",
       "      <td>0.338625</td>\n",
       "      <td>0.717486</td>\n",
       "      <td>0.835371</td>\n",
       "      <td>0.771954</td>\n",
       "      <td>0.857633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.303273</td>\n",
       "      <td>0.786898</td>\n",
       "      <td>0.836023</td>\n",
       "      <td>0.810717</td>\n",
       "      <td>0.887395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.286675</td>\n",
       "      <td>0.761999</td>\n",
       "      <td>0.882478</td>\n",
       "      <td>0.817825</td>\n",
       "      <td>0.886596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3990, training_loss=0.3152834096349272, metrics={'train_runtime': 6785.8115, 'train_samples_per_second': 75.223, 'train_steps_per_second': 0.588, 'total_flos': 6.761758624175923e+16, 'train_loss': 0.3152834096349272, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91     14961\n",
      "           1       0.76      0.88      0.82      6308\n",
      "\n",
      "    accuracy                           0.88     21269\n",
      "   macro avg       0.85      0.88      0.86     21269\n",
      "weighted avg       0.89      0.88      0.88     21269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Testing\"\"\"\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_path = \"/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_situations_and_comments_custom_trainer\"\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\"\"\"Predict\"\"\"\n",
    "predictions_output = trainer.predict(tokenized_dataset['test'])\n",
    "\n",
    "\n",
    "preds = np.argmax(predictions_output.predictions, axis=-1)\n",
    "labels = predictions_output.label_ids\n",
    "print(classification_report(y_true=labels, y_pred=preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model for p(y|c,rot) -> ROT-moral_foundations and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170149/170149 [02:07<00:00, 1332.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:15<00:00, 1368.72 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:15<00:00, 1355.07 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 170149\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenized_dataset = social_comments_dataset.map(tokenize_data_moralFoundations_comments)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_social_norms/bert_comments_classification_rot-moralFoundations_and_comments_custom_trainer\",\n",
    "    learning_rate=2e-5,\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119945 training samples have label 0 (NTA), 50204 training samples have label 1 (YTA)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2951, 0.7049])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "We have 119945 training samples with label 0 (NTA) and 50204 training samples with label 1 (YTA). \n",
    "A clear disproportion, thats why we need to assign different weights to the labels when computing the loss. \n",
    "The label with the least amount of data (label 1 in our case), should have the higher weight.\n",
    "\"\"\"\n",
    "\n",
    "def get_samples_per_class(labels):\n",
    "    return torch.bincount(labels)\n",
    "\n",
    "samples_per_class_train = get_samples_per_class(torch.Tensor(tokenized_dataset[\"train\"]['labels']).int())\n",
    "print(f\"{samples_per_class_train[0]} training samples have label 0 (NTA), {samples_per_class_train[1]} training samples have label 1 (YTA)\")\n",
    "\n",
    "total_training_samples = samples_per_class_train.sum()\n",
    "weights = torch.Tensor( [samples_per_class_train[1]/total_training_samples, samples_per_class_train[0]/total_training_samples] )\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "\"\"\"Create Custom Trainer to assign different weights to classes when computing the loss due to the large data imbalance\"\"\"\n",
    "class CustomTrainer(Trainer):\n",
    "    #override the compute_loss function of the Trainer class\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights.to(model.module.device)) # weights is a 1D Tensor assigning weight to each of the classes\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgjergjplepi12\u001b[0m (\u001b[33msocial-chem-101-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/IAIS/gplepi/entero/wandb/run-20240120_170710-nuf7uupl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/social-chem-101-team/huggingface/runs/nuf7uupl' target=\"_blank\">leafy-oath-41</a></strong> to <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/social-chem-101-team/huggingface/runs/nuf7uupl' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface/runs/nuf7uupl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3990' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3990/3990 1:53:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.456300</td>\n",
       "      <td>0.416587</td>\n",
       "      <td>0.644847</td>\n",
       "      <td>0.765933</td>\n",
       "      <td>0.700194</td>\n",
       "      <td>0.810804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>0.389641</td>\n",
       "      <td>0.669064</td>\n",
       "      <td>0.803749</td>\n",
       "      <td>0.730248</td>\n",
       "      <td>0.828718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.322400</td>\n",
       "      <td>0.374641</td>\n",
       "      <td>0.663636</td>\n",
       "      <td>0.832926</td>\n",
       "      <td>0.738706</td>\n",
       "      <td>0.830034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3990, training_loss=0.38457842375102796, metrics={'train_runtime': 6808.1265, 'train_samples_per_second': 74.976, 'train_steps_per_second': 0.586, 'total_flos': 6.761758624175923e+16, 'train_loss': 0.38457842375102796, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.83      0.87     14961\n",
      "           1       0.67      0.84      0.74      6308\n",
      "\n",
      "    accuracy                           0.83     21269\n",
      "   macro avg       0.80      0.83      0.81     21269\n",
      "weighted avg       0.85      0.83      0.83     21269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_path = \"/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_rot-moralFoundations_and_comments_custom_trainer\"\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\"\"\"Predict\"\"\"\n",
    "predictions_output = trainer.predict(tokenized_dataset['test'])\n",
    "\n",
    "\n",
    "preds = np.argmax(predictions_output.predictions, axis=-1)\n",
    "labels = predictions_output.label_ids\n",
    "print(classification_report(y_true=labels, y_pred=preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model for p(y|c,rot) -> ROT-categories and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170149/170149 [02:12<00:00, 1283.24 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:16<00:00, 1291.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:17<00:00, 1250.76 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 170149\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenized_dataset = social_comments_dataset.map(tokenize_data_categories_comments)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_social_norms/bert_comments_classification_rot-categories_and_comments_custom_trainer\",\n",
    "    learning_rate=2e-5,\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119945 training samples have label 0 (NTA), 50204 training samples have label 1 (YTA)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2951, 0.7049])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "We have 119945 training samples with label 0 (NTA) and 50204 training samples with label 1 (YTA). \n",
    "A clear disproportion, thats why we need to assign different weights to the labels when computing the loss. \n",
    "The label with the least amount of data (label 1 in our case), should have the higher weight.\n",
    "\"\"\"\n",
    "\n",
    "def get_samples_per_class(labels):\n",
    "    return torch.bincount(labels)\n",
    "\n",
    "samples_per_class_train = get_samples_per_class(torch.Tensor(tokenized_dataset[\"train\"]['labels']).int())\n",
    "print(f\"{samples_per_class_train[0]} training samples have label 0 (NTA), {samples_per_class_train[1]} training samples have label 1 (YTA)\")\n",
    "\n",
    "total_training_samples = samples_per_class_train.sum()\n",
    "weights = torch.Tensor( [samples_per_class_train[1]/total_training_samples, samples_per_class_train[0]/total_training_samples] )\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "\"\"\"Create Custom Trainer to assign different weights to classes when computing the loss due to the large data imbalance\"\"\"\n",
    "class CustomTrainer(Trainer):\n",
    "    #override the compute_loss function of the Trainer class\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights.to(model.module.device)) # weights is a 1D Tensor assigning weight to each of the classes\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgjergjplepi12\u001b[0m (\u001b[33msocial-chem-101-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/IAIS/gplepi/entero/wandb/run-20240120_204721-6mpluh7s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/social-chem-101-team/huggingface/runs/6mpluh7s' target=\"_blank\">lunar-universe-42</a></strong> to <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/social-chem-101-team/huggingface/runs/6mpluh7s' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface/runs/6mpluh7s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3990' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3990/3990 1:53:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.415414</td>\n",
       "      <td>0.636038</td>\n",
       "      <td>0.781907</td>\n",
       "      <td>0.701470</td>\n",
       "      <td>0.808030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.362400</td>\n",
       "      <td>0.388972</td>\n",
       "      <td>0.678642</td>\n",
       "      <td>0.788264</td>\n",
       "      <td>0.729357</td>\n",
       "      <td>0.831257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>0.382069</td>\n",
       "      <td>0.658092</td>\n",
       "      <td>0.829829</td>\n",
       "      <td>0.734049</td>\n",
       "      <td>0.826555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3990, training_loss=0.38267225298965185, metrics={'train_runtime': 6832.7009, 'train_samples_per_second': 74.706, 'train_steps_per_second': 0.584, 'total_flos': 6.761758624175923e+16, 'train_loss': 0.38267225298965185, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing\"\"\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_rot-categories_and_comments_custom_trainer\"\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.0028272 ,  2.348375  ],\n",
       "       [-2.521682  ,  2.969072  ],\n",
       "       [ 3.3939176 , -3.3149705 ],\n",
       "       ...,\n",
       "       [ 2.5599544 , -2.472094  ],\n",
       "       [ 2.024031  , -2.026351  ],\n",
       "       [ 0.4161413 , -0.27432227]], dtype=float32), label_ids=array([0, 1, 0, ..., 0, 0, 0]), metrics={'test_loss': 0.38528215885162354, 'test_precision': 0.6633863234923039, 'test_recall': 0.8335447051363348, 'test_f1': 0.7387944358578052, 'test_accuracy': 0.8251915933988434, 'test_runtime': 110.4458, 'test_samples_per_second': 192.574, 'test_steps_per_second': 1.512})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output = trainer.predict(tokenized_dataset['test'])\n",
    "predictions_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.82      0.87     14961\n",
      "           1       0.66      0.83      0.74      6308\n",
      "\n",
      "    accuracy                           0.83     21269\n",
      "   macro avg       0.79      0.83      0.80     21269\n",
      "weighted avg       0.84      0.83      0.83     21269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = np.argmax(predictions_output.predictions, axis=-1)\n",
    "labels = predictions_output.label_ids\n",
    "print(classification_report(y_true=labels, y_pred=preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model for p(y|c,a) -> Author ID and Comment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170149/170149 [02:14<00:00, 1266.52 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:15<00:00, 1348.63 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:15<00:00, 1384.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 170149\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenized_dataset = social_comments_dataset.map(tokenize_data_authorId_comments)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_social_norms/bert_comments_classification_authorId_and_comments_custom_trainer\",\n",
    "    learning_rate=2e-5,\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119945 training samples have label 0 (NTA), 50204 training samples have label 1 (YTA)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2951, 0.7049])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "We have 119945 training samples with label 0 (NTA) and 50204 training samples with label 1 (YTA). \n",
    "A clear disproportion, thats why we need to assign different weights to the labels when computing the loss. \n",
    "The label with the least amount of data (label 1 in our case), should have the higher weight.\n",
    "\"\"\"\n",
    "\n",
    "def get_samples_per_class(labels):\n",
    "    return torch.bincount(labels)\n",
    "\n",
    "samples_per_class_train = get_samples_per_class(torch.Tensor(tokenized_dataset[\"train\"]['labels']).int())\n",
    "print(f\"{samples_per_class_train[0]} training samples have label 0 (NTA), {samples_per_class_train[1]} training samples have label 1 (YTA)\")\n",
    "\n",
    "total_training_samples = samples_per_class_train.sum()\n",
    "weights = torch.Tensor( [samples_per_class_train[1]/total_training_samples, samples_per_class_train[0]/total_training_samples] )\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "\"\"\"Create Custom Trainer to assign different weights to classes when computing the loss due to the large data imbalance\"\"\"\n",
    "class CustomTrainer(Trainer):\n",
    "    #override the compute_loss function of the Trainer class\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights.to(model.module.device)) # weights is a 1D Tensor assigning weight to each of the classes\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgjergjplepi12\u001b[0m (\u001b[33msocial-chem-101-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/IAIS/gplepi/entero/wandb/run-20240120_230915-mnte1sy2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/social-chem-101-team/huggingface/runs/mnte1sy2' target=\"_blank\">pious-mountain-43</a></strong> to <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/social-chem-101-team/huggingface/runs/mnte1sy2' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface/runs/mnte1sy2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3990' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3990/3990 1:54:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>0.419288</td>\n",
       "      <td>0.615029</td>\n",
       "      <td>0.803097</td>\n",
       "      <td>0.696593</td>\n",
       "      <td>0.798204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>0.395703</td>\n",
       "      <td>0.693018</td>\n",
       "      <td>0.770171</td>\n",
       "      <td>0.729561</td>\n",
       "      <td>0.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.378603</td>\n",
       "      <td>0.645400</td>\n",
       "      <td>0.839283</td>\n",
       "      <td>0.729682</td>\n",
       "      <td>0.820631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3990, training_loss=0.3858281453450521, metrics={'train_runtime': 6859.5815, 'train_samples_per_second': 74.414, 'train_steps_per_second': 0.582, 'total_flos': 6.761758624175923e+16, 'train_loss': 0.3858281453450521, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.86     14961\n",
      "           1       0.65      0.85      0.74      6308\n",
      "\n",
      "    accuracy                           0.82     21269\n",
      "   macro avg       0.79      0.83      0.80     21269\n",
      "weighted avg       0.84      0.82      0.82     21269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Testing\"\"\"\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_path = \"/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_authorId_and_comments_custom_trainer\"\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\"\"\"Predict\"\"\"\n",
    "predictions_output = trainer.predict(tokenized_dataset['test'])\n",
    "\n",
    "\n",
    "preds = np.argmax(predictions_output.predictions, axis=-1)\n",
    "labels = predictions_output.label_ids\n",
    "print(classification_report(y_true=labels, y_pred=preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model for p(y|c,s,a) -> situation and Author ID and Comment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170149/170149 [02:09<00:00, 1318.47 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:15<00:00, 1336.26 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21269/21269 [00:16<00:00, 1292.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 170149\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Tokenize the data\"\"\"\n",
    "tokenized_dataset = social_comments_dataset.map(tokenize_data_situation_authorId_comments)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_social_norms/bert_comments_classification_situations_authorId_and_comments_custom_trainer\",\n",
    "    learning_rate=2e-5,\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119945 training samples have label 0 (NTA), 50204 training samples have label 1 (YTA)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2951, 0.7049])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "We have 119945 training samples with label 0 (NTA) and 50204 training samples with label 1 (YTA). \n",
    "A clear disproportion, thats why we need to assign different weights to the labels when computing the loss. \n",
    "The label with the least amount of data (label 1 in our case), should have the higher weight.\n",
    "\"\"\"\n",
    "\n",
    "def get_samples_per_class(labels):\n",
    "    return torch.bincount(labels)\n",
    "\n",
    "samples_per_class_train = get_samples_per_class(torch.Tensor(tokenized_dataset[\"train\"]['labels']).int())\n",
    "print(f\"{samples_per_class_train[0]} training samples have label 0 (NTA), {samples_per_class_train[1]} training samples have label 1 (YTA)\")\n",
    "\n",
    "total_training_samples = samples_per_class_train.sum()\n",
    "weights = torch.Tensor( [samples_per_class_train[1]/total_training_samples, samples_per_class_train[0]/total_training_samples] )\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\"\"\"Create Custom Trainer to assign different weights to classes when computing the loss due to the large data imbalance\"\"\"\n",
    "class CustomTrainer(Trainer):\n",
    "    #override the compute_loss function of the Trainer class\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights.to(model.module.device)) # weights is a 1D Tensor assigning weight to each of the classes\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgjergjplepi12\u001b[0m (\u001b[33msocial-chem-101-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/IAIS/gplepi/entero/wandb/run-20240121_012523-pbo66680</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/social-chem-101-team/huggingface/runs/pbo66680' target=\"_blank\">logical-snowball-44</a></strong> to <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/social-chem-101-team/huggingface' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/social-chem-101-team/huggingface/runs/pbo66680' target=\"_blank\">https://wandb.ai/social-chem-101-team/huggingface/runs/pbo66680</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3990' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3990/3990 1:54:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.346978</td>\n",
       "      <td>0.704694</td>\n",
       "      <td>0.824613</td>\n",
       "      <td>0.759952</td>\n",
       "      <td>0.849734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>0.304890</td>\n",
       "      <td>0.760704</td>\n",
       "      <td>0.845640</td>\n",
       "      <td>0.800926</td>\n",
       "      <td>0.878744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.295045</td>\n",
       "      <td>0.721679</td>\n",
       "      <td>0.902363</td>\n",
       "      <td>0.801970</td>\n",
       "      <td>0.871456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3990, training_loss=0.32085349099677907, metrics={'train_runtime': 6897.4579, 'train_samples_per_second': 74.005, 'train_steps_per_second': 0.578, 'total_flos': 6.761758624175923e+16, 'train_loss': 0.32085349099677907, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90     14961\n",
      "           1       0.72      0.90      0.80      6308\n",
      "\n",
      "    accuracy                           0.87     21269\n",
      "   macro avg       0.84      0.88      0.85     21269\n",
      "weighted avg       0.88      0.87      0.87     21269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Testing\"\"\"\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_path = \"/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_situations_authorId_and_comments_custom_trainer\"\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\"\"\"Predict\"\"\"\n",
    "predictions_output = trainer.predict(tokenized_dataset['test'])\n",
    "\n",
    "\n",
    "preds = np.argmax(predictions_output.predictions, axis=-1)\n",
    "labels = predictions_output.label_ids\n",
    "print(classification_report(y_true=labels, y_pred=preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If it's for your best, than do not worry about it. -> [{'label': 'NTA', 'score': 0.9414990544319153}]\n",
      "You shouldn't have done that, it's not allowed. -> [{'label': 'YTA', 'score': 0.8737062811851501}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Only comments\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_path = '/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_custom_trainer'\n",
    "\n",
    "nta_comment = \"If it's for your best, than do not worry about it.\"\n",
    "yta_comment = \"You shouldn't have done that, it's not allowed.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=model_path, tokenizer=tokenizer)\n",
    "\n",
    "answer = clf(nta_comment)\n",
    "print(f\"{nta_comment} -> {answer}\")\n",
    "\n",
    "answer = clf(yta_comment)\n",
    "print(f\"{yta_comment} -> {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the large increace in certainty when predicting a YTA label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The situations is: Want to study abroad, but feel bad leaving my country.\n",
      "The comments:\n",
      "If it's for your best, than do not worry about it. -> [{'label': 'NTA', 'score': 0.9377086162567139}]\n",
      "You shouldn't have done that, it's not good behaviour. -> [{'label': 'NTA', 'score': 0.9377086162567139}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Situations and comments\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_path = '/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_situations_and_comments_custom_trainer/checkpoint-1000/'\n",
    "\n",
    "situation = \"Want to study abroad, but feel bad leaving my country.\"\n",
    "nta_comment = \"If it's for your best, than do not worry about it.\"\n",
    "yta_comment = \"You shouldn't have done that, it's not good behaviour.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=model_path, tokenizer=tokenizer)\n",
    "\n",
    "print(f\"The situations is: {situation}\")\n",
    "print(\"The comments:\")\n",
    "\n",
    "answer = clf(situation + \" \" + tokenizer.sep_token + \" \" + nta_comment)\n",
    "print(f\"{nta_comment} -> {answer}\")\n",
    "\n",
    "answer = clf(situation + \" \" + tokenizer.sep_token + \" \" + nta_comment)\n",
    "print(f\"{yta_comment} -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If it's for your best, than do not worry about it. -> [{'label': 'NTA', 'score': 0.9722619652748108}]\n",
      "You shouldn't have done that, it's not allowed. -> [{'label': 'YTA', 'score': 0.8665910363197327}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"rot-moralFoundations and comments\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_path = '/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_rot-moralFoundations_and_comments_custom_trainer'\n",
    "\n",
    "rot_moralFoundation = 'loyalty-betrayal'\n",
    "nta_comment = \"If it's for your best, than do not worry about it.\"\n",
    "yta_comment = \"You shouldn't have done that, it's not allowed.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=model_path, tokenizer=tokenizer)\n",
    "\n",
    "answer = clf(rot_moralFoundation + \" \" + tokenizer.sep_token + \" \" + nta_comment)\n",
    "print(f\"{nta_comment} -> {answer}\")\n",
    "\n",
    "answer = clf(rot_moralFoundation + \" \" + tokenizer.sep_token + \" \" + yta_comment)\n",
    "print(f\"{yta_comment} -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If it's for your best, than do not worry about it. -> [{'label': 'NTA', 'score': 0.9723081588745117}]\n",
      "You shouldn't have done that, it's not allowed. -> [{'label': 'YTA', 'score': 0.8744572401046753}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"rot-categories and comments\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_path = '/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_rot-categories_and_comments_custom_trainer'\n",
    "\n",
    "rot_category = 'morality-ethics'\n",
    "nta_comment = \"If it's for your best, than do not worry about it.\"\n",
    "yta_comment = \"You shouldn't have done that, it's not allowed.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=model_path, tokenizer=tokenizer)\n",
    "\n",
    "answer = clf(rot_category + \" \" + tokenizer.sep_token + \" \" + nta_comment)\n",
    "print(f\"{nta_comment} -> {answer}\")\n",
    "\n",
    "answer = clf(rot_category + \" \" + tokenizer.sep_token + \" \" + yta_comment)\n",
    "print(f\"{yta_comment} -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If it's for your best, than do not worry about it. -> [{'label': 'NTA', 'score': 0.9316185116767883}]\n",
      "You shouldn't have done that, it's not allowed. -> [{'label': 'YTA', 'score': 0.8692306876182556}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"AuthorId and comments\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_path = '/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_authorId_and_comments_custom_trainer'\n",
    "\n",
    "authorId = 'ei9ofvo'\n",
    "nta_comment = \"If it's for your best, than do not worry about it.\"\n",
    "yta_comment = \"You shouldn't have done that, it's not allowed.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=model_path, tokenizer=tokenizer)\n",
    "\n",
    "answer = clf(authorId + \". \" + nta_comment)\n",
    "print(f\"{nta_comment} -> {answer}\")\n",
    "\n",
    "answer = clf(authorId + \". \" + yta_comment)\n",
    "print(f\"{yta_comment} -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/gplepi/anaconda3/envs/entero_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If it's for your best, than do not worry about it. -> [{'label': 'NTA', 'score': 0.9864906072616577}]\n",
      "You shouldn't have done that, it's not allowed. -> [{'label': 'NTA', 'score': 0.8591297268867493}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Situations and AuthorId and comments\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_path = '/home/IAIS/gplepi/entero/output_social_norms/bert_comments_classification_situations_authorId_and_comments_custom_trainer'\n",
    "\n",
    "authorId = 'ei9ofvo'\n",
    "situation = \"Want to study abroad, but feel bad leaving my country.\"\n",
    "\n",
    "nta_comment = \"If it's for your best, than do not worry about it.\"\n",
    "yta_comment = \"You shouldn't have done that, it's not allowed.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=model_path, tokenizer=tokenizer)\n",
    "\n",
    "answer = clf( situation + \" \" + tokenizer.sep_token + \" \" + (authorId + \". \" + nta_comment) )\n",
    "print(f\"{nta_comment} -> {answer}\")\n",
    "\n",
    "answer = clf( situation + \" \" + tokenizer.sep_token + \" \" +  (authorId + \". \" + yta_comment) )\n",
    "print(f\"{yta_comment} -> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entero_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
